---
title: "Whack a MoG"
author: "Verdini - Romeo (1765820 - 1618216)"
date: "15/1/2021"
output: html_document

---

<style type="text/css">
h1.title {
  font-size: 60px;
  color: Navy;
  text-align: center;
  font-weight: bold;
}
h4.author { 
  font-size: 18px;
  #font-family: "Times New Roman", Times, serif;
  color: Black;
  text-align: left;
  font-weight: bold;
}
h4.date { 
  font-size: 18px;
  #font-family: "Times New Roman", Times, serif;
  color: Black;
  text-align: left;
  font-weight: bold;
}
</style>



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
<br><br>

### <span style="color: purple;"> 1) Libraries, Parameters & Useful functions </span>


As first thing, here are reported the used external libraries to accomplish the task.  
```{r include=TRUE, message=FALSE, eval=TRUE}
suppressMessages(require(mixtools, quietly = T))
suppressMessages(require(KScorrect, quietly = T))
suppressMessages(require(caret, quietly = T))
library(gt)
library(readr)
library(tidyverse)
library(kableExtra)
library(hrbrthemes)
library(viridis)
library(forcats)
library(ggplot2)
library(gridExtra)

```


The gen_distr() is the function that will generate samples from a Bart distribution implemented using the rnormix() function implemented in R.  
```{r include=TRUE, message=FALSE, eval=FALSE}
gen_distr <- function(n, M){
  distr = list()
  for (j in 1:M) distr[[j]] = rnormmix(n, lambda = c(0.5, rep(0.1,5)),
                                       mu = c(0, ((0:4)/2)-1), sigma = c(1, rep(0.1,5)) )
  return(distr)
}
```


Here below are reported the initialization parameters that the user can change in order to get different results. 
```{r include=TRUE, message=FALSE, eval=FALSE}
set.seed(42)

init_num=8 #Every function that calls the EM algorithm tries to initialize multiple times 
#(for a fixed number of gaussians) and keeps the parameters that gave the best results
#on the training sample (which for AIC and BIC coincides with the test sample).
#Set this to 1 for a (much) faster run of the code.

n_small=10 # number of small samples from the Bart.
n_large=10 # number of large samples from the Bart.

k_max=12 # The number of gaussians used for the MoG will be from 1 to k_max.

small_n = gen_distr(250,n_small) #One can choose how many points will be in every small sample
large_n = gen_distr(5000,n_large) #One can choose how many points will be in every large sample

data=c(small_n,large_n) #We concatenate the lists of datasets
```

The fallowing 4 functions will be used during the execution of the code.

* podium = this function will be called at the end of every metric to return the top k values chosen by every metric on every sample.

* rep.row = it's our function that repeat a vector n times, returning a matrix with n copies of the input vectors as rows. 

* sum.columns = it's our implementation of colSums()- Given a matrix as input, it sums all the columns, returning an array where in position i will be sums of columns i.

* likefunction = this function takes in input a dataset and the results of the hem_fit function and gives the log_likelihood associated to the MoG with the parameters learned by the EM algorithm on the given data.
```{r include=TRUE, message=FALSE, eval=FALSE}
podium <- function(vec,size=3){
  copied_vec = vec # make copy
  result <- rep(NA,size) # initialize vec in memory
  for (i in 1:size){#iterate until the podium is full filled
    idx = which.min(copied_vec) # get the smallest
    result[i] = which.min(copied_vec)  # add it to the vec
    copied_vec[idx] = Inf # substitute it with Inf
  }
  return(result)
}


rep.row<-function(x,n){
  matrix(rep(x,each=n),nrow=n)
} # the function is used to pre-allocate the matrix 

# the function is used to return a vector where each entry is the sum of each element
# in that column.
sum.columns <- function(A){
  res_vec <- rep(0, ncol(A))
  for (i in 1:nrow(A)){
    res_vec <- res_vec + A[i,]  
  }
  return(res_vec)
} 


#This function takes in input a dataset and the results of the hem_fit function and gives the log_likelihood associated to the MoG with the parameters learned by the EM algorithm on the given data.
likefunction <- function(y, hem_fit){
  n_gauss = length(hem_fit$parameters)/3
  p = hem_fit$parameters[1:n_gauss]
  mu = hem_fit$parameters[(n_gauss+1):(n_gauss*2)]
  sigma = hem_fit$parameters[(n_gauss*2+1):(n_gauss*3)]
  like <- 0
  for (i in 1:n_gauss){
    like <- like + p[i]*dnorm(y, mu[i], sigma[i])
  }
  return(log(like))
}

```

### <span style="color: purple;"> 2) EM Function</span>

This is the adjusted handmade.em function that works with k Gaussians. We have decided to stick with the code ([Brutti's implementation](https://elearning.uniroma1.it/pluginfile.php/389598/course/section/166527/Topic%2013%20-%20Parametric%20Inference%20-%20MoG.R?time=1608629571077)) implemented by prof. Brutti,  with the fallowing changes:

* k = is the input parameter that changes the number of Gaussians. 

* m = is the input parameter used while plotting the functions to identify which on dataset the function is optimizing the parameters.

* likefunction = it is a function that computes the likelihood on the whole given dataset.

* d_tot, r_tot = correspond to r1, r2, d1 and d2 but, since we are dealing with more than two Gaussinas, they are matrices instead of numbers.

Moreover, all the other parameters (p, mu, sigma) have been treated as matrices since there are more than two Gaussians. 
Note: We left the calculation of the likelihood and the deviance in the handmade.em() function even though we don't use them. They could be removed to speed up the computation.  

We have considered to implement an early stopping condition by looking how the deviance changes throughout the iterations of the algorithm, but at the end we decided to simply use 200 iterations  because sometimes the speed of convergence is slow and after more iterations it is possible to observe a significant improvement. Look at [Appendix A](#A) for an example of this.


```{r include=TRUE, message=FALSE, eval=FALSE}

handmade.em <- function(y, p, mu, sigma, n_iter, plot_flag = F, k, m){
  
  #function to compute the likelihood function
  likefunction <- function(y){
    like <- 0
    for (i in 1:k){
      like <- like + p[i]*dnorm(y, mu[i], sigma[i])
    }
    return(like)
  }
  
  like <- likefunction(y) #compute likelihood
  like[like<1e-300]=1e-300 #sometimes we get NaN when this number is 0 or really close to   it.
  #It happens rarely, most often when one of the points is really far from the           distribution.
  deviance <- -2*sum(log(like)) # compute deviance
  
  res      <- matrix(NA,n_iter + 1, 2+3*k)
  res[1,]  <- c(0, p, mu, sigma, deviance)
  
  d_tot <- rep.row(rep(NA,length(y)),k)
  r_tot <- rep.row(rep(NA,length(y)),k)
  
  for (iter in 1:n_iter) {
    
    # E step (get responsibilities)
    # responsibility = proportion times the Gaussian over the 2 found parameters
    for (j in 1:k) {d_tot[j,] <- pmax(p[j]*dnorm(y, mu[j], sigma[j]),1e-300)}
    # getting optimal hidden state (since it is proportionality we need to normalize)      distribution
    for (j in 1:k) {r_tot[j,] <- d_tot[j,]/sum.columns(d_tot)} 
    
    # M step
    # here we compute the p, mean and sigma for each gaussian.
    for (j in 1:k){
      r = r_tot[j,]
      p[j]     <- mean(r)
      mu[j]    <- sum(r*y)/(sum(r))   
      sigma[j] <- max(sqrt( sum(r*(y^2))/sum(r) - (mu[j])^2 ),1.0e-8)   #We don't want
      #a value of sigma too small to avoid computational errors.
      if (is.nan(sigma[j])) {
        sigma[j]=0.05
        print("NaN found in sigma[j]. Value reinitialized to 0.05") 
        #Sometimes the sqrt in sigma[j] returns NaN. We decided to reinitialize that           sigma
        #value to 0.05 and hope it doesn't give NaN the second time.
      }
      }
    
    # -2 x log-likelihood (a.k.a. deviance)
    like <- pmax(likefunction(y),1e-30)    # update likelihood 
    deviance <- -2*sum( log(like) )# update deviance
    
    # Save
    res[iter+1,] <- c(iter, p, mu, sigma, deviance)
    
    # Plot
    if (plot_flag && iter==n_iter){
      hist(y, prob = T, breaks=100, 
           col = "pink", border = "white",
           main = paste("Bart Simpson Density, M=",m),
           xlab = paste("EM Iteration: ", iter, "/", n_iter, sep = ""))

      
     # add curves, colors and legend
      cols = rep(NA,k+1) 
      cols[k+1] = "orange"
      # #iterate to plot every distribution
      for (i in 1:k) {
        cols[i] = rgb(runif(1),runif(1),runif(1),.8)
        curve(p[i]*dnorm(x,mu[i],sigma[i]),
                            lwd = 3,
                            lty = 5,
                            col = cols[i],
                            add = TRUE)}
      # add the join distribution
      curve(likefunction(x),
            lwd = 4, col = "orange", add = TRUE)
      # add the legend
      legendlabels=rep(NA,k+1)
      for (i in 1:k){legendlabels[i]=paste("Distribution ", i)}
      legendlabels[k+1] = "joint "
      
      legend('topright',legend=legendlabels,
             col=cols, lty=1, cex=0.6)
      grid()
      
    }
  }
  res <- data.frame(res)
  out <- list(parameters = c(p = p, mu = mu, sigma = sigma),
              deviance = deviance, 
              res = res)
  return(out)
}
```

### <span style="color: purple;">3) Multiple Initialization</span>

Since the Mixture of Gaussians EM is very sensitive to the initialization parameters, there are multiple solutions that can be considered like:

* trying multiple initialization and keeping only the one that returns best results after the fit. 

AND/OR

* finding an initialization method that returns good results more consistently. One could try using something like k-means++ initialization on the $\mu$ or, provided that one knows additional information about the data, it is possible to use a more specific initialization method. For example, in our case one could initialize one gaussian centered in zero and with a large $\sigma$ and then spread the others along the interval [-1;-1].

We have decided to follow the easiest way and not using additional information about the data. i.e. we initialize with simple uniform distribution for the parameters ($\mu$, $\sigma$) and we do that multiple times and keeping as results only the one that return a better results in terms of fitting. Instead, regarding the p parameter, we didn't randomize its initialization but we assigned 1/k, where k is the number of Gaussians.

To understand why a good initialization is important, in [Appendix B](#B), we show 5 different results of the handmade.em() function, with different (random) initialization parameters. Notice how the one with the highest mean log-likelihood is the one that also better fits the data visually. It is important to highlight that the choice of initialization parameters is done on training set and not on the test. 

### <span style="color: purple;"> 4) AIC & BIC</span>

The AIC_BIC() function represented below is the function used to score every MoG with the two metrics AIC and BIC. The function, after having optimized the parameter by calling the handmade.em() function, computes the AIC and BIC scores. It keeps, by calling the podium(), the top three k values (representing the number of gaussian forming each MoG) according to the AIC and BIC metrics.

```{r include=TRUE, message=FALSE, eval=FALSE, class.source="bg-success"}

AIC_BIC <-function(Dataset,n_iter=500){
  AIC_models=list()
  BIC_models=list()
  
  AIC_results=rep(0,k_max)
  BIC_results=rep(0,k_max)

  
  for (j in 1:length(Dataset)){ #This cycle iterates over all samples 
    XX <- Dataset[[j]]
    n <- length(XX) 
    
    for (i in 1:k_max){ #This is the cycle that tries different number of gaussians
      
      #We now do a for loop to find the best initialization data for a particular
      #fixed k (number of gaussians) and for a fixed dataset XX.
      
      ll_sum <- -Inf  #This is a default initialization value. Every likelihood will be       better than this.
      
      for (w in 1:init_num){
        
        hem_fit_temp <- handmade.em(XX, 
                               p      = rep(1/i,i), 
                               mu     = runif(i,min=-1.5,max=1.5),
                               sigma  = runif(i,min=0.1,max=0.4), 
                               n_iter = n_iter,
                               plot_flag = F,
                               k=i,
                               m=j)
        
        ll_temp<- likefunction(XX, hem_fit_temp) # compute log-likelihood on XX.
        
        if (sum(ll_temp) > ll_sum) { # if the new ll is better than the best we had              before
          ll <- ll_temp    # save new ll in the ll variable
          ll_sum <- sum(ll) # also compute the sum for the next iteration
          hem_fit <- hem_fit_temp # and save all the values returned by the EM                   algorithm.
        }
        
      }
      n_par=length(hem_fit$parameters)-1 # if one knows n-1 values for p, the last one       is forced
      #to be 1-(sum(p_i)). This means that there's one less degree of freedom in the         possible
      #values of p.
      AIC <- round(-2*sum(ll)+2*n_par,2) # compute AIC
      BIC <- round(-2*sum(ll) + log(n)*n_par,2) # compute BIC
      
      AIC_results[length(hem_fit$parameters)/3]=AIC 
      BIC_results[length(hem_fit$parameters)/3]=BIC 
      
    }
    AIC_models[[j]] <- podium(AIC_results)
    BIC_models[[j]] <- podium(BIC_results)
    remove(XX)
  }
  
  out <- list(AIC = AIC_models, 
              BIC = BIC_models)
  return (out)
}
```

The AIC_results, BIC_results are two lists that store the AIC and BIC results.

```{r include=TRUE, message=FALSE, eval=FALSE, class.source="bg-success"}
temp_result=AIC_BIC(data,n_iter=200)

AIC_results=temp_result$AIC
BIC_results=temp_result$BIC
```


### <span style="color: purple;"> 5) Sample Splitting </span>

The sample_splitting() function takes in input the train_size (30%, 50% or 70% in our cases), all the data on which it has to work, and the number of iterations. Once it has returned the optimized parameters on the training sample by calling the handmade.em() function, it computes the log-likelihood on the train, and if it was better than the one computed in the previous iteration, it also computes the log-likelihood on the test. At the end it returns, by calling the podium(), the top three k values (representing the number of gaussian forming each MoG) according to log-likelihood computed on the test set.

```{r include=TRUE, message=FALSE, eval=FALSE, class.source="bg-success"}
#The structure of the code here is similar to the on in the AIC_BIC() function above.
sample_splitting <-function(train_size,Dataset,n_iter=500) {
  models=list()
  for (j in 1:length(Dataset))
    {
    dataset=Dataset[[j]]
    trainIndex<-createDataPartition(dataset, p = train_size, 
                        list = FALSE, 
                        times = 1) #We use the function "createDataPartition" from the                                        library "caret" to split the dataset in train and test.
    train <- dataset[trainIndex]
    test <- dataset[-trainIndex]
    
    results=rep(0,k_max) #array that will hold the results
    
    for (i in 1:k_max){ #cycle over the different number of gaussians.
      
      ll_sum <- -Inf
      
      #This is the cycle that will look for the best initialization data on the
      #TRAIN set for a fixed k (number of gaussians) value.
      for (w in 1:init_num)
        {
        hem_fit_temp <- handmade.em(train, 
                                    p      = rep(1/i,i), 
                                    mu     = runif(i,min=-1.5,max=1.5),
                                    sigma  = runif(i,min=0.1,max=0.4), 
                                    n_iter = n_iter,
                                    plot_flag = F,
                                    k=i,
                                    m=j) #the em is called on the train sample.
        
        
        #We now compute the log likelihood with TRAIN
        ll_train<- likefunction(train, hem_fit_temp) # compute log-likelihood on the TRAIN
        
        if (sum(ll_train) > ll_sum) #if it was better than the last init parameters on 
                                    #train then:
          {
            ll <- likefunction(test, hem_fit_temp) # compute log-likelihood on the TEST
            ll_sum <- sum(ll_train) #keep new best value for the ll on TRAIN
            hem_fit <- hem_fit_temp #keep all the resutls from this em run.
          }
      }
      #Now ll corresponds to the log-likelihood of the test dataset with the                 initialization data that had the best results on the train set
      results[length(hem_fit$parameters)/3]= -sum(ll) #store results for different k values in an array. We put "-" because our podium function takes the least 3 values.

    }
    
    models[[j]] <- podium(results) #keep best 3 results.
   
  }
  return (models)
}
```

Below are reported three lists containing the results for different train-test ratios.
```{r include=TRUE, message=FALSE, eval=FALSE, class.source="bg-success"}
Sample_splitting30_results=sample_splitting(0.30,data,n_iter=100) 
Sample_splitting50_results=sample_splitting(0.50,data,n_iter=100)
Sample_splitting70_results=sample_splitting(0.70,data,n_iter=100)
```


### <span style="color: purple;"> 6) Cross Validation </span>

The cv() function, is our implementation of CV-sample splitting. It keeps in input:
* x = a dataset.

* k_fold = the number of folds on which we want to our dataset to split into.
It retuns a list with the desired number of folds.
```{r include=TRUE, message=FALSE, eval=FALSE, class.source="bg-success"}
cv <- function(x, k_fold){
  dataset <- x # copy the data set
  l = length(dataset) # get its length
  folds <- list() # list will contain all folds
  size <- as.integer(l/k_fold) # size of each fold
  for (n in 1:k_fold){ # iterate over all the desired folders
    fold_elements = c() # vector that will store elements
    while (length(fold_elements) < size){ # iterate until the fold size is reached
      idx_elm = sample(1:l, 1) # get index element
      fold_elements = c(fold_elements, dataset[idx_elm]) # add element to the vector
      dataset = dataset[-idx_elm] # drop element from data set
      l = length(dataset) # recompute the length
    }
    folds[[n]] = fold_elements  # add vector(representing the folder) to the folder list
  }
  if (l != 0){# if there are still data points
    for (m in 1:l){ #assign them one by one to the folders until are done
      folds[[m]] = c(folds[[m]], dataset[1]) # assign a data point to folder m
      dataset = dataset[-1] # delete it from the data set
    }
  }
  return(folds)
}

```


The cross_validation() function takes in input:
* Dataset = a dataset

* k_folds = the number of folds on which we want to our dataset to split into. This parameter is necessary since this function calls internally the cv() function previously described.

*n_iter = the number of iteration for the handmade.em() function

As always, every time we do a fit we try different initialization and we return the one with the best log-likelihood. Of course, in this case we do that every time the validation test changes. For each k (where k always represent the number of gaussians composing the MoG) we have in our hand 5 or 10 log-likelihood computed on every different validation sets (depending if we are dealing with 5-CV or 10-CV), then we compute the average and we store it into an array called results. We finally call the podium() function to return the top three k values (representing the number of gaussian forming each MoG) according to the average log-likelihood computed on the validation sets.

```{r include=TRUE, message=FALSE, eval=FALSE, class.source="bg-success"}

#The structure of this function is similar to the sample splitting.

cross_validation<-function(Dataset,k_folds,n_iter){
  models=list()
  for (j in 1:length(Dataset))
  {
    dataset=Dataset[[j]] 
    k_cv=cv(dataset, k_folds) #we create the folds with our function
    
    results=rep(0,k_max)   #array to store the results
    
    for (i in 1:k_max) 
    { 
      
      
      k_cv_results=rep(NA,k_folds) #array to store the results for a particular fold
      
      for (fold in 1:k_folds) #for each fold
        {
        test=k_cv[[fold]]  #split into test
        train=do.call(c,k_cv[-fold]) #and train
        
        
        ll_sum <- -Inf #initialize the "best likelihood". This will change when better initialziation parameters are found.
        
        #This is the cycle to find better initialization parameters.
        for (w in 1:init_num)
        {
          hem_fit_temp <- handmade.em(train, 
                                      p      = rep(1/i,i), 
                                      mu     = runif(i,min=-1.5,max=1.5),
                                      sigma  = runif(i,min=0.1,max=0.4), 
                                      n_iter = n_iter,
                                      plot_flag = F,
                                      k=i,
                                      m=j)
          ll_train<- likefunction(train, hem_fit_temp) # compute log-likelihood with                                                          TRAIN
          if (sum(ll_train) > ll_sum) 
          {
            ll <- likefunction(test, hem_fit_temp) # compute log-likelihood with TEST
            ll_sum <- sum(ll_train) #save best likelihood on TRAIN found until now
            hem_fit <- hem_fit_temp  #and the returned parameters for that particular                                            fitted model
          }
        }
        #Now ll corresponds to the log-likelihood of the test dataset with the                 inizialization data that had the best results on the train set
        
        k_cv_results[fold]=sum(ll_test) #store the result in this array
        
        
      }
      ll_mean=mean(k_cv_results) #get the mean log-likelihood over the k folds. 
      results[length(hem_fit$parameters)/3]= -ll_mean #store mean in this array. We put
                                  # "-" because our podium function takes lowest 3 values.
      
    }
    models[[j]] <- podium(results) #call our function to get top 3 for each dataset.
      
  }
  return (models)
}
```

Below are reported two lists containing the results for the two different CVs.
```{r include=TRUE, message=FALSE, eval=FALSE, class.source="bg-success"}
Cross_validation5_results=cross_validation(data,5,n_iter=100)
Cross_validation10_results=cross_validation(data,10,n_iter=100)
```


### <span style="color: purple;"> 7) Wasserstein score </span>


```{r include=TRUE, message=FALSE, eval=FALSE, class.source="bg-success"}
Wasserstein_score <-function(Dataset,n_iter=500) {
  models=list()
  for (j in 1:length(Dataset))
  
  {
    results=rep(0,k_max) #array that will keep the results for each k
    
    dataset=Dataset[[j]]
    trainIndex<-createDataPartition(dataset, p = 0.5, 
                                    list = FALSE, 
                                    times = 1)
    train <- dataset[trainIndex] #split into train
    test <- dataset[-trainIndex] #and test
    
    #We define a function of a single variable. It corresponds to the quantile of our
    #empirical distribution. We use the test dataset for this.
    Quantile_to_integrate_te <-function(z) quantile(test,prob=z,names=FALSE)
    
    
    for (i in 1:k_max){
      
      ll_sum <- -Inf #initialize the "best likelihood". It will be update when better initialization parameters are found for a particular k.
      
      #This cycle is looking for the best initialization parameters for a particular k.
      for (w in 1:init_num)
      { hem_fit_temp <- handmade.em(train, 
                                    p      = rep(1/i,i), 
                                    mu     = runif(i,min=-1.5,max=1.5),
                                    sigma  = runif(i,min=0.1,max=0.4), 
                                    n_iter = n_iter,
                                    plot_flag = F,
                                    k=i,
                                    m=j)
        ll_train<- likefunction(train, hem_fit_temp) # compute log-likelihood with TRAIN
        
        if (sum(ll_train) > ll_sum) #if we found a better log likelihood
        {
          ll <- ll_train #keep that ll
          ll_sum <- sum(ll_train) #the sum for further comparisons
          hem_fit <- hem_fit_temp #and all the returned parameters.
        }
      }
      #Now ll corresponds to the log-likelihood of the train dataset with the inizialization 
      #data that had the best results on the train set
      
      #get the p, menas and sigma values.
      len=length(hem_fit$parameters)
      prob=hem_fit$parameters[1:(len/3)]
      means=hem_fit$parameters[(1+len/3):(2*len/3)]
      sigmas=hem_fit$parameters[(2*len/3+1):len]
      
      #define the quantile for a MoG with k values with the parameters got by em_fit
      Quantile_to_integrate_k <-function(z) qmixnorm(z, mean=means, pro=prob, sd=sigmas,expand=0.10)
      
      #integrate the absolute value of the difference between 0 and 1.
      function_to_integrate <-function(z) abs(Quantile_to_integrate_te(z)-Quantile_to_integrate_k(z))
      W_k=integrate(function_to_integrate,lower=0,upper=1,subdivisions = 10000)
      
      #and store the result in the array "results"
      results[length(hem_fit$parameters)/3]=W_k$value
    }
    
    models[[j]] <- podium(results) #keep top 3 models for a particular dataset.
    
  }
  return (models)
  
}
```

Below is returned the lists containing the results based on the Wasserstain score.
```{r include=TRUE, message=FALSE, eval=FALSE, class.source="bg-success"}
Wasserstein_score_results=Wasserstein_score(data,n_iter=100)
```


```{r include=TRUE, message=FALSE, eval=FALSE, class.source="bg-success"}
listHolder=list(AIC=AIC_results,
                BIC=BIC_results,
                Sample_splitting30=Sample_splitting30_results,
                Sample_splitting50=Sample_splitting50_results,
                Sample_splitting70=Sample_splitting70_results,
                Cross_validation5=Cross_validation5_results,
                Cross_validation10=Cross_validation10_results,
                Wesserstein_score=Wasserstein_score_results
                )
TO_SAVE= do.call(rbind, listHolder)
write.csv2(TO_SAVE,'listHolder.csv')
print("finished")
```

### <span style="color: purple;"> 8) Results</span> 

We now comment the result we got. A full table is in [Appendix C](C).

* The "true" value for k=6 is almost never in the top 3. With our method of initialization, it's really hard for the model to actually find good parameters for a fit with 6 gaussians and even when it does it's not necessarily better than a model with more gaussians according to the particular metric (AIC, BIC, log-likelihood and Wasserstein score). The number 7 is best result when we are dealing with n=5000: the EM-fit-Algorithm usually leads to two gaussians with large sigma and small p and 5 different gaussians with small sigmas centered at the peaks. 

* The k value chosen by the AIC is always higher or equal than the one chosen from the BIC for both sample sizes. This is expected since the penalization that the BIC gives to the number of parameters used is higher and the fit for small datasets is not very good. The effect is still there for the large sample size, but it's less visible and the number of chosen gaussians is often the same. AIC and BIC are the two metrics that return results closer to the "ideal" one (k=6).

* On the large datasets, the models chosen by the sample splitting  with 50% and 70% as the train set choose higher values of k than the ones that the 30% Sample splitting does. We didn't expect this result, but by looking further into it and from the fact the 30% train-70% test gets valuer closer to 7 we realize that it's probably a consequence of only using the log-likelihood as a metric without penalizing the number of parameters used in the model. We also found out, by doing some other tests with following the results got in the 30-70 split, that having large test set in general leads to slightly better results.

* We chose to return the top 3 results for each metric, but this representation doesn't show if the top 3 models are close (with the metric given by that particular scoring method) or not. Both cases happen in our tests. It's worth noting, though, that for all the methods, except for the last one, the scores are on a logarithmic scale: even a "small" difference from the top scoring one can be significant and could be sufficient to discard the model. As an example, if one were to normalize the AIC scores that we calculated and got scores of 1000 and 1050, while one cannot conclude that the first model is a "good" one, it is possible to say that according to the AIC the second model is significantly worse (the probability of it being better according to AIC is proportional to $e^{-50/2}$). A more accurate analysis could be done by keeping the top k values but only if the difference from the top one is over a certain threshold.

```{r include=TRUE, echo = FALSE, message=FALSE, eval=TRUE, warning=FALSE, class.source="bg-success"}
listHolder2 <- read_delim("listHolder2.csv", 
    ";", escape_double = FALSE, trim_ws = TRUE)
```


```{r include=TRUE, echo = FALSE, message=FALSE, warning=FALSE, eval=TRUE, class.source="bg-success"}

m_names = c("AIC","BIC","SamS_30","SamS_50","SamS_70","CV_5","CV_10","Wesserstein")

list_res = list() # list where will be stored dfs
c=1
for (d in listHolder2[-1]){#iterate over every column
  mid_matrix = matrix(NA, nrow = 8, ncol = 4) # matrix where will be stored the results
  for (v in 1:length(d)){ # open a loop that goes from 1 to 8 
    get_value = eval(parse(text=d[v])) # get the value into num
    get_value = append(get_value, m_names[v],after=0) # append to the vector the method name
    mid_matrix[v,] = get_value # add the vector to the matrix
  }
  df = as.data.frame(mid_matrix)
  names(df) = c("Method",
                paste("1st(",c,")",sep=""),
                paste("2nd(",c,")",sep=""),
                paste("3rd(",c,")",sep=""))
  list_res[[c]] = df
  c = c+1
}
```


```{r include=TRUE, echo = FALSE, message=FALSE, eval=TRUE, class.source="bg-success"}

small_matrix = matrix(NA, nrow = 240, ncol = 2) #matrix storing top 3 for small n
big_matrix = matrix(NA, nrow = 240, ncol = 2) #matrix storing top 3 for big n
small_first_matrix = matrix(NA, nrow = 80, ncol = 2) #matrix storing top 1 for small n
big_first_matrix = matrix(NA, nrow = 80, ncol = 2) #matrix storing top 1 for small n
small_c = 1 # counter for top 3 small n
big_c = 1 # counter for top 3 big n
small_c_first = 1 # counter for top 1 small n 
big_c_first = 1 # counter for top 1 big n
  
for (n in 1:8){
  m = length(listHolder2) #length == 20
  get_model = listHolder2[n,2:m] # get a row of the listHolder file
  threshold = 1 # threshould to distringuish between results from small and big n
  for (v in get_model){ #iterate over each vector
    get_value = eval(parse(text=v)) # convert from string to number
    if (threshold<=10){ # if results belong to the small n
      small_first_matrix[small_c_first,] = append(get_value[1], m_names[n],after=0) # add to
      # the small_first matrix only the 1st result
      small_c_first = small_c_first +1
      for (e in get_value){#iterate over the the 3 results
        e = append(e, m_names[n],after=0) # add the method that returned those results
        small_matrix[small_c,] = e # add the vector to the small_matrix
        small_c = small_c+1
      }
    }
    else { # if results belong to big n
        big_first_matrix[big_c_first,] = append(get_value[1], m_names[n],after=0)# add to
        # the big_first matrix only the 1st result
        big_c_first = big_c_first +1
        for (e in get_value){#iterate over the the 3 results
          e = append(e, m_names[n],after=0)# add the method that returned those results
          big_matrix[big_c,] = e# add the vector to the big_matrix
          big_c = big_c+1
        }
    }
    threshold = threshold +1
  }
}  

```

```{r include=TRUE, echo = FALSE, message=FALSE, eval=TRUE, class.source="bg-success"}
# function to convert a matrix into a df
sort_results <- function(matrix_to_convert){
  df= as.data.frame(matrix_to_convert) #convert into df
  names(df) = c("Method", "Result") 
  
  #add sort levels
  df$Result <- factor(df$Result,levels = c("1", "2", "3", "4",
                                           "5", "6", "7", "8","9","10","11", "12"))
  return(df)
}

# function to plot histogram either for top 3 or top 1 results:
single_hist <- function(matrix_to_convert){
  df = sort_results(matrix_to_convert) # call the sort function
  p <- df %>%
    ggplot( aes(x=Result, color=Method, fill=Method)) +
    geom_histogram(alpha=0.6, binwidth = 5, stat = 'count') 
  return(p)
}

# function to plot multiple histogram only of top 3 results:
multiple_hist <- function(matrix_to_convert,sample_s){
  df = sort_results(matrix_to_convert)
  p<- df %>%
    ggplot( aes(x=Result, color=Method, fill=Method)) +
    geom_histogram(alpha=0.6, binwidth = 5, stat = 'count') +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
    theme_ipsum() +
    theme(
      legend.position="none",
      panel.spacing = unit(1, "lines"),
      strip.text.x = element_text(size = 10,face='bold'),
      axis.title.x = element_text(hjust=.5, vjust=-1.5,colour='red',face='bold'),
      axis.title.y = element_text(hjust=.5, vjust=1.5,colour='red',face='bold'),
      plot.title = element_text(color='purple', size=12)
    ) +
    ggtitle(paste("Metric Comporison for sample size n =  ", sample_s, sep="")) +
    xlab("K") +
    ylab("Count") +
    
    facet_wrap(~Method)
  
  return(p)
}

```


```{r include=TRUE, echo=FALSE, warning=FALSE}
grid.arrange(single_hist(small_first_matrix),
             single_hist(big_first_matrix), nrow=1, ncol=2)
```

```{r include=TRUE, echo=FALSE, warning=FALSE}
grid.arrange(single_hist(small_matrix),
             single_hist(big_matrix), nrow=1, ncol=2)
```

```{r include=TRUE, echo=FALSE, warning=FALSE}
multiple_hist(small_matrix, 300)
multiple_hist(big_matrix, 5000)
```





### <span style="color: CornflowerBlue;"> 7) Appendix A</span> {#A}

The graph on the left shows the whole optimization by looking at the deviance, whereas the graph on the right is the same but without the first iteration. This allows us to show the phenomenon we mentioned in Section 2.
```{r, echo=FALSE,out.width="49%",  out.height="20%",fig.cap="Deviance",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("images/deviance01.png" ,"images/deviance02.png"))
``` 



### <span style="color: CornflowerBlue;"> 7) Appendix B</span> {#B}


```{r, echo=FALSE,out.width="49%",  out.height="20%",fig.cap="Fit examples",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("images/Rplot01.png" ,"images/Rplot02.png", "images/Rplot03.png", "images/Rplot04.png", "images/Rplot.png"))

``` 



### <span style="color: CornflowerBlue;"> 7) Appendix C</span> {#C}
```{r, eval=TRUE, echo=FALSE}
# Here we use the kable library to represent the results of the small n with a nice table
small_df = list_res[1:10] %>% reduce(left_join, by = "Method")
kbl(small_df) %>% kable_classic() %>% 
  add_header_above(c(" " = 1, "Small Sample 1" = 3, "Small Sample 2" = 3, 
                     "Small Sample 3" = 3, "Small Sample 4" = 3, "Small Sample 5" = 3,
                     "Small Sample 6" = 3, "Small Sample 7" = 3, "Small Sample 8" = 3,
                     "Small Sample 9" = 3, "Small Sample 10" = 3), bold = T)  %>%
  column_spec(c(2,5,8,11,14,17,20,23,26,29), background = "gold") %>% 
  column_spec(c(3,6,9,12,15,18,21,24,27,30), background = "lightcyan") %>% 
  column_spec(c(4,7,10,13,16,19,22,25,28,31), background = "coral") %>%
  column_spec(1, bold=T, color = 'red') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), position = "left")
```

```{r, eval=TRUE, echo=FALSE}
# Here we use the kable library to represent the results of the big n with a nice table
big_df = list_res[11:20] %>% reduce(left_join, by = "Method")
kbl(big_df) %>% kable_classic() %>% 
  add_header_above(c(" " = 1, "Big Sample 11" = 3, "Big Sample 12" = 3, 
                     "Big Sample 13" = 3, "Big Sample 14" = 3, "Big Sample 15" = 3,
                     "Big Sample 16" = 3, "Big Sample 17" = 3, "Big Sample 18" = 3,
                     "Big Sample 19" = 3, "Big Sample 20" = 3), bold = T)  %>%
  column_spec(c(2,5,8,11,14,17,20,23,26,29), background = "gold") %>% 
  column_spec(c(3,6,9,12,15,18,21,24,27,30), background = "lightcyan") %>% 
  column_spec(c(4,7,10,13,16,19,22,25,28,31), background = "coral") %>%
  column_spec(1, bold=T, color = 'red') %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), position = "left")
```





